{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c42179-aa89-46a2-9ad5-1847537d01b4",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "# Define the model architecture\n",
    "def instance_segmentation_model(S, C, HI, WI):\n",
    "    inputs = tf.keras.layers.Input(shape=(None, None, 3))\n",
    "    # Divide input image into SxS uniform grids\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.image.extract_patches(x, [1, S, S, 1], [1, S, S, 1], [1, 1, 1, 1], 'SAME'))(inputs)\n",
    "    # Semantic category prediction for each grid cell\n",
    "    semantic = tf.keras.layers.Conv2D(C, 1)(x)\n",
    "    # Position-sensitive encoding of pixel coordinates\n",
    "    coords = tf.range(S, dtype=tf.float32) / (S - 1)\n",
    "    x_coords, y_coords = tf.meshgrid(coords, coords)\n",
    "    x_coords = tf.tile(tf.reshape(x_coords, (1, S, S, 1)), [tf.shape(inputs)[0], 1, 1, 1])\n",
    "    y_coords = tf.tile(tf.reshape(y_coords, (1, S, S, 1)), [tf.shape(inputs)[0], 1, 1, 1])\n",
    "    x = tf.concat([x, x_coords, y_coords], axis=-1)\n",
    "    # Instance mask generation for each grid cell\n",
    "    x = tf.keras.layers.Conv2D(HI*WI*S*S, 1)(x)\n",
    "    x = tf.keras.layers.Reshape((S*S, HI*WI))(x)\n",
    "    outputs = tf.keras.layers.Concatenate(axis=-1)([semantic, x])\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Define input image dimensions and grid size\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "grid_size = 5\n",
    "mask_size = 32\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = tf.keras.layers.Input(shape=(input_height, input_width, 3))\n",
    "\n",
    "# Add a normalization layer to the input tensor\n",
    "normalized_input = tf.keras.layers.Lambda(lambda x: x / 255.0 - 0.5)(input_tensor)\n",
    "\n",
    "# Add a CoordConv layer to the input tensor\n",
    "coordconv_layer = tf.keras.layers.Lambda(lambda x: tf.stack(tf.meshgrid(tf.linspace(-1.0, 1.0, x.shape[1]), \n",
    "                                                            tf.linspace(-1.0, 1.0, x.shape[2])), \n",
    "                                             axis=-1))(input_tensor)\n",
    "input_features = tf.keras.layers.Concatenate(axis=-1)([normalized_input, coordconv_layer])\n",
    "\n",
    "# Add the backbone network to the input features\n",
    "backbone = tf.keras.applications.ResNet50(input_tensor=input_features, include_top=False)\n",
    "\n",
    "# Add the feature pyramid network (FPN) to the backbone output\n",
    "fpn = tf.keras.models.Model(inputs=backbone.input, outputs=backbone.get_layer('conv5_block3_out').output)\n",
    "fpn_output = fpn.output\n",
    "\n",
    "# Add the instance segmentation heads to each FPN feature level\n",
    "mask_outputs = []\n",
    "for i in range(3, 6):\n",
    "    # Instance category prediction branch\n",
    "    category_branch = tf.keras.layers.Conv2D(80, (3, 3), padding='same', activation='relu', name=f'category_branch_{i}')(fpn.get_layer(f'conv2_block{i}_out').output)\n",
    "    category_output = tf.keras.layers.Conv2D(grid_size ** 2, (1, 1), padding='valid', activation='sigmoid', name=f'category_output_{i}')(category_branch)\n",
    "\n",
    "    # Instance mask prediction branch\n",
    "    mask_branch = tf.keras.layers.Conv2D(80, (3, 3), padding='same', activation='relu', name=f'mask_branch_{i}')(fpn.get_layer(f'conv2_block{i}_out').output)\n",
    "    mask_output = tf.keras.layers.Conv2D(grid_size ** 2 * mask_size, (1, 1), padding='valid', activation='sigmoid', name=f'mask_output_{i}')(mask_branch)\n",
    "\n",
    "    mask_outputs.append(mask_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3f30e6-ebf0-400d-8965-df529f395fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Define the number of grid cells, the number of classes, and the image size\n",
    "S = 5\n",
    "C = 10\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "# Define the input layer for the image\n",
    "input_layer = tf.keras.layers.Input(shape=(H, W, 3))\n",
    "\n",
    "# Define the pixel coordinate layer\n",
    "coord_layer = tf.keras.layers.Lambda(lambda x: tf.meshgrid(tf.linspace(-1.0, 1.0, H), tf.linspace(-1.0, 1.0, W)))(input_layer)\n",
    "coord_layer = tf.keras.layers.Lambda(lambda x: tf.stack(x, axis=-1))(coord_layer)\n",
    "coord_layer = tf.keras.layers.Reshape((H, W, 2))(coord_layer)\n",
    "coord_layer = tf.keras.layers.Lambda(lambda x: x * 10.0)(coord_layer)  # Scale the coordinates to match the feature map\n",
    "\n",
    "# Define the backbone network (not shown in the diagram)\n",
    "backbone = ...\n",
    "\n",
    "# Define the feature pyramid network (not shown in the diagram)\n",
    "fpn = ...\n",
    "\n",
    "# Apply the backbone network to the input image\n",
    "x = backbone(input_layer)\n",
    "\n",
    "# Apply the feature pyramid network to the backbone output\n",
    "features = fpn(x)\n",
    "\n",
    "# Concatenate the pixel coordinate layer with the feature map\n",
    "features = tf.keras.layers.Concatenate(axis=-1)([features, coord_layer])\n",
    "\n",
    "# Create the category prediction head\n",
    "category_prediction = tf.keras.layers.Conv2D(C, kernel_size=3, padding='same', activation='softmax')(features)\n",
    "\n",
    "# Create the instance mask generation head\n",
    "instance_mask = tf.keras.layers.Conv2D(S*S, kernel_size=3, padding='same', activation='sigmoid')(features)\n",
    "\n",
    "# Reshape the output of the instance mask generation head to HI×WI×S^2\n",
    "instance_mask = tf.keras.layers.Reshape((H, W, S*S))(instance_mask)\n",
    "\n",
    "# Define a lambda layer to extract the instance mask at each grid cell\n",
    "mask_extraction_layer = tf.keras.layers.Lambda(lambda x: tf.stack(\n",
    "    [tf.gather(tf.transpose(x, perm=[2, 0, 1]), i) for i in range(S*S)], axis=-1))\n",
    "\n",
    "# Apply the mask extraction layer to the instance mask\n",
    "instance_mask = mask_extraction_layer(instance_mask)\n",
    "\n",
    "# Multiply the category prediction by the instance mask to get the final output\n",
    "output = tf.keras.layers.Multiply()([category_prediction, instance_mask])\n",
    "\n",
    "# Define the model with the input and output layers\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "\n",
    "\n",
    "def CoordConv(x):\n",
    "    \"\"\"Implementation of CoordConv operation.\"\"\"\n",
    "    batch_size, height, width, channels = tf.shape(x)\n",
    "    x_coords = tf.range(-1, 1, 2/height, dtype=tf.float32)\n",
    "    y_coords = tf.range(-1, 1, 2/width, dtype=tf.float32)\n",
    "    x_coords = tf.tile(tf.reshape(x_coords, [1, height, 1, 1]), [batch_size, 1, width, 1])\n",
    "    y_coords = tf.tile(tf.reshape(y_coords, [1, 1, width, 1]), [batch_size, height, 1, 1])\n",
    "    return tf.concat([x, x_coords, y_coords], axis=3)\n",
    "\n",
    "def build_SOLO_head(input_tensor, num_classes):\n",
    "    \"\"\"Builds the SOLO head network architecture.\"\"\"\n",
    "    # Feature pyramid network (FPN)\n",
    "    C3, C4, C5 = tf.keras.applications.ResNet50(include_top=False, weights=None).outputs\n",
    "    P3 = C3\n",
    "    P4 = tf.keras.layers.Add()([tf.keras.layers.UpSampling2D()(P3), C4])\n",
    "    P5 = tf.keras.layers.Add()([tf.keras.layers.UpSampling2D()(P4), C5])\n",
    "    P3 = tf.keras.layers.Conv2D(256, kernel_size=1, padding='same')(P3)\n",
    "    P4 = tf.keras.layers.Conv2D(256, kernel_size=1, padding='same')(P4)\n",
    "    P5 = tf.keras.layers.Conv2D(256, kernel_size=1, padding='same')(P5)\n",
    "    P6 = tf.keras.layers.MaxPooling2D(pool_size=1, strides=2, padding='same')(P5)\n",
    "    features = [P3, P4, P5, P6]\n",
    "\n",
    "    # Category prediction sub-network\n",
    "    cls_preds = []\n",
    "    for feature in features:\n",
    "        cls_pred = tf.keras.layers.Conv2D(num_classes, kernel_size=3, padding='same')(feature)\n",
    "        cls_pred = tf.keras.layers.Activation('sigmoid')(cls_pred)\n",
    "        cls_preds.append(cls_pred)\n",
    "\n",
    "    # Instance mask segmentation sub-network\n",
    "    mask_preds = []\n",
    "    for feature in features:\n",
    "        mask_feat = CoordConv(feature)\n",
    "        mask_feat = tf.keras.layers.Conv2D(256, kernel_size=3, padding='same')(mask_feat)\n",
    "        mask_feat = tf.keras.layers.ReLU()(mask_feat)\n",
    "        mask_feat = tf.keras.layers.Conv2D(256, kernel_size=3, padding='same')(mask_feat)\n",
    "        mask_feat = tf.keras.layers.ReLU()(mask_feat)\n",
    "        mask_pred = tf.keras.layers.Conv2D(1, kernel_size=1, activation='sigmoid')(mask_feat)\n",
    "        mask_preds.append(mask_pred)\n",
    "\n",
    "    # Upsampling and concatenation\n",
    "    for i in range(len(mask_preds)):\n",
    "        if i == 0:\n",
    "            mask_preds[i] = tf.keras.layers.UpSampling2D()(mask_preds[i])\n",
    "        else:\n",
    "            mask_preds[i] = tf.keras.layers.UpSampling2D()(mask_preds[i])\n",
    "            mask_preds[i] = tf.keras.layers.Add()([mask_preds[i], mask_preds[i-1]])\n",
    "    mask_preds[-1] = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=2, strides=2, padding='same')(mask_preds[-1])\n",
    "    mask_preds[-1] = tf.keras.layers.Activation('sigmoid')(mask_preds[-1])\n",
    "    mask_preds.reverse()\n",
    "\n",
    "    # SOLO head output tensor\n",
    "    solo_output = tf.keras.layers.Concatenate(axis=-1)([category_output, mask_output])\n",
    "    \n",
    "    \n",
    "    # Reshape the SOLO output tensor to (batch_size, num_grids, output_height, output_width, num_classes+4)\n",
    "    solo_output = tf.reshape(solo_output, [batch_size, num_grids, output_height, output_width, num_classes+4])\n",
    "\n",
    "    # Gather the raw instance segmentation results\n",
    "    raw_results = tf.gather_nd(solo_output, indices)\n",
    "\n",
    "    # Apply sigmoid to the category scores\n",
    "    category_scores = tf.sigmoid(raw_results[..., :num_classes])\n",
    "\n",
    "    # Apply softmax to the category scores\n",
    "    category_scores = tf.nn.softmax(category_scores, axis=-1)\n",
    "\n",
    "    # Apply sigmoid to the mask predictions\n",
    "    mask_predictions = tf.sigmoid(raw_results[..., num_classes:])\n",
    "\n",
    "    # Apply NMS to obtain the final instance segmentation results\n",
    "    instance_masks = []\n",
    "    for i in range(batch_size):\n",
    "        # Obtain the category scores and mask predictions for the i-th image\n",
    "        category_scores_i = category_scores[i, ...]\n",
    "        mask_predictions_i = mask_predictions[i, ...]\n",
    "\n",
    "        # Apply NMS to the predictions for the i-th image\n",
    "        nms_indices = tf.image.non_max_suppression(\n",
    "            boxes=grid_boxes,\n",
    "            scores=tf.reshape(category_scores_i, [-1]),\n",
    "            max_output_size=max_instances_per_grid,\n",
    "            iou_threshold=nms_threshold\n",
    "        )\n",
    "\n",
    "        # Gather the mask predictions for the selected indices\n",
    "        nms_masks = tf.gather(mask_predictions_i, nms_indices, axis=-1)\n",
    "\n",
    "        # Resize the masks to the original image size\n",
    "        nms_masks = tf.image.resize(\n",
    "            nms_masks,\n",
    "            size=(image_height, image_width),\n",
    "            method=tf.image.ResizeMethod.BILINEAR\n",
    "        )\n",
    "\n",
    "        instance_masks.append(nms_masks)\n",
    "\n",
    "    # Stack the instance masks for all images in the batch\n",
    "    instance_masks = tf.stack(instance_masks, axis=0)\n",
    "\n",
    "    return instance_masks\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"Computes the Dice Loss between the ground truth masks (y_true) and the predicted masks (y_pred)\"\"\"\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=(1, 2))\n",
    "    return 1 - (numerator + 1) / (denominator + 1)\n",
    "\n",
    "def solo_loss(y_true, y_pred):\n",
    "    \"\"\"Computes the SOLO training loss, including both the categorical loss and the mask loss\"\"\"\n",
    "    Lcate = tf.keras.losses.CategoricalCrossentropy()(y_true[:, :, :, :-2], y_pred[:, :, :, :-2])\n",
    "    Npos = tf.reduce_sum(tf.cast(tf.reduce_max(y_true[:, :, :, :-2], axis=-1) > 0, tf.float32))\n",
    "    mask_targets = tf.reshape(y_true[:, :, :, -2:], [-1, tf.shape(y_true)[1] * tf.shape(y_true)[2], 2])\n",
    "    mask_preds = tf.reshape(y_pred[:, :, :, -2:], [-1, tf.shape(y_pred)[1] * tf.shape(y_pred)[2], 2])\n",
    "    Lmask = tf.reduce_sum(dice_loss(mask_targets[:, :, 1], mask_preds[:, :, 1]) * mask_targets[:, :, 0]) / (Npos + 1e-6)\n",
    "    return Lcate + 3 * Lmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a0ba8d-f0f3-4f68-bf8c-1ebb9a0e2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, MaxPooling2D, Conv2DTranspose, Dropout, BatchNormalization, Activation, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "               name=f'res{stage}{block}_branch2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=3, name=f'bn{stage}{block}_branch2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=f'res{stage}{block}_branch2b')(x)\n",
    "    x = BatchNormalization(axis=3, name=f'bn{stage}{block}_branch2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=f'res{stage}{block}_branch2c')(x)\n",
    "    x = BatchNormalization(axis=3, name=f'bn{stage}{block}_branch2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      name=f'res{stage}{block}_branch1')(input_tensor)\n",
    "    shortcut = BatchNormalization(\n",
    "        axis=3, name=f'bn{stage}{block}_branch1')(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"\n",
    "    identity block 구현\n",
    "\n",
    "    Arguments:\n",
    "    input_tensor -- 입력 tensor\n",
    "    kernel_size -- middle layer conv filter 크기\n",
    "    filters -- tuple 형태로 각 conv layer filter 수\n",
    "    stage -- layer 그룹\n",
    "    block -- 블록 이름\n",
    "\n",
    "    Returns:\n",
    "    output tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # conv layer들의 이름 지정\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # filter 수\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    # Shortcut path\n",
    "    x_shortcut = input_tensor\n",
    "\n",
    "    # 1st conv layer\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 2nd conv layer\n",
    "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=3, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 3rd conv layer\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=3, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    # shortcut과 합치기\n",
    "    x = Add()([x, x_shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def FPN50(inputs):\n",
    "    # Backbone - ResNet50\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(inputs)\n",
    "    x = BatchNormalization(axis=3, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    c2 = conv_block(x, stage=2, block='a')\n",
    "    c3 = identity_block(x, stage=3, block='a')\n",
    "    c4 = identity_block(x, stage=4, block='a')\n",
    "    c5 = identity_block(x, stage=5, block='a')\n",
    "\n",
    "    # Feature Pyramid Network\n",
    "    p5 = Conv2D(256, (1, 1), name='fpn_c5p5')(c5)\n",
    "    p4 = Concatenate()([Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', name='fpn_p5upsampled')(p5), Conv2D(256, (1, 1), name='fpn_c4p4')(c4)])\n",
    "    p3 = Concatenate()([Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', name='fpn_p4upsampled')(p4), Conv2D(256, (1, 1), name='fpn_c3p3')(c3)])\n",
    "    p2 = Concatenate()([Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', name='fpn_p3upsampled')(p3), Conv2D(256, (1, 1), name='fpn_c2p2')(c2)])\n",
    "\n",
    "    # Top-down pathway\n",
    "    p5 = Conv2D(256, (3, 3), padding='same', name='fpn_p5')(p5)\n",
    "    p4 = Conv2D(256, (3, 3), padding='same', name='fpn_p4')(p4)\n",
    "    p3 = Conv2D(256, (3, 3), padding='same', name='fpn_p3')(p3)\n",
    "    p2 = Conv2D(256, (3, 3), padding='same', name='fpn_p2')(p2)\n",
    "    \n",
    "    # Mask branch\n",
    "    x = p5\n",
    "    for i in range(4):\n",
    "        x = Conv2D(256, (3, 3), padding='same', name=f'mask_conv{i}')(x)\n",
    "        x = BatchNormalization(name=f'mask_bn{i}')(x)\n",
    "        x = Activation('relu')(x)\n",
    "    mask_output = Conv2D(1, (1, 1), activation='sigmoid', name='mask_output')(x)\n",
    "\n",
    "    # Class branch\n",
    "    x = p5\n",
    "    for i in range(4):\n",
    "        x = Conv2D(256, (3, 3), padding='same', name=f'class_conv{i}')(x)\n",
    "        x = BatchNormalization(name=f'class_bn{i}')(x)\n",
    "        x = Activation('relu')(x)\n",
    "    class_output = Conv2D(NUM_CLASSES, (1, 1), activation='softmax', name='class_output')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=inputs, outputs=[mask_output, class_output], name='FPN50')\n",
    "    return model\n",
    "\n",
    "\n",
    "def dice_loss_fn(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    dice_loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return dice_loss\n",
    "\n",
    "\n",
    "def train(model, train_dataset, optimizer, epoch, max_epoch):\n",
    "    # Define loss objects\n",
    "    focal_loss = tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    dice_loss = dice_loss_fn\n",
    "\n",
    "    # Define lambda function for combining loss\n",
    "    def combined_loss(y_true, y_pred):\n",
    "        categorical_loss = focal_loss(y_true[:, :, :, 0], y_pred[:, :, :, 0])\n",
    "        mask_loss = dice_loss(y_true[:, :, :, 1:], y_pred[:, :, :, 1:])\n",
    "        return categorical_loss + 3 * mask_loss\n",
    "\n",
    "    # Define metrics for monitoring training\n",
    "    categorical_loss_metric = tf.keras.metrics.Mean(name='categorical_loss')\n",
    "    mask_loss_metric = tf.keras.metrics.Mean(name='mask_loss')\n",
    "    total_loss_metric = tf.keras.metrics.Mean(name='total_loss')\n",
    "\n",
    "    # Iterate over dataset\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataset):\n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=True)\n",
    "            loss = combined_loss(targets, predictions)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Update metrics\n",
    "        categorical_loss_metric.update_state(focal_loss(targets[:, :, :, 0], predictions[:, :, :, 0]))\n",
    "        mask_loss_metric.update_state(dice_loss(targets[:, :, :, 1:], predictions[:, :, :, 1:]))\n",
    "        total_loss_metric.update_state(loss)\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch {}/{} Batch {}/{} Categorical loss {:.4f} Mask loss {:.4f} Total loss {:.4f}'\n",
    "                  .format(epoch + 1, max_epoch, batch_idx, len(train_dataset),\n",
    "                          categorical_loss_metric.result(), mask_loss_metric.result(), total_loss_metric.result()))\n",
    "\n",
    "    # Reset metrics\n",
    "    categorical_loss_metric.reset_states()\n",
    "    mask_loss_metric.reset_states()\n",
    "    total_loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3fffd51-feea-462c-8555-4b970ace3802",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 256, 256, 256), (None, 128, 128, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5768\\3796033605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Feature Pyramid Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mp5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_c5p5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mp4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_p5upsampled'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_c4p4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_p4upsampled'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_c3p3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_p3upsampled'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fpn_c2p2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 970\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1106\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1108\u001b[1;33m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    876\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2623\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2624\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2625\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2626\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2627\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    514\u001b[0m             shape[axis] for shape in shape_set if shape[axis] is not None)\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_dims\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 256, 256, 256), (None, 128, 128, 256)]"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(512,512, 3))\n",
    "#def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)\n",
    "               \n",
    "               \n",
    "# Backbone - ResNet50\n",
    "x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(inputs)\n",
    "x = BatchNormalization(axis=3, name='bn_conv1')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "# Residual blocks\n",
    "c2 = conv_block(x, kernel_size=(7,7), filters=(16,32,64), stage=2, block='a')\n",
    "c3 = identity_block(x, kernel_size=(7,7), filters=(16,32,64), stage=3, block='a')\n",
    "c4 = identity_block(x, kernel_size=(7,7), filters=(16,32,64), stage=4, block='a')\n",
    "c5 = identity_block(x, kernel_size=(7,7), filters=(16,32,64), stage=5, block='a')\n",
    "\n",
    "# Feature Pyramid Network\n",
    "p5 = Conv2D(256, (1, 1), name='fpn_c5p5')(c5)\n",
    "p4 = Concatenate()([Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', name='fpn_p5upsampled')(p5), Conv2D(256, (1, 1), name='fpn_c4p4')(c4)])\n",
    "p3 = Concatenate()([Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', name='fpn_p4upsampled')(p4), Conv2D(256, (1, 1), name='fpn_c3p3')(c3)])\n",
    "p2 = Concatenate()([Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', name='fpn_p3upsampled')(p3), Conv2D(256, (1, 1), name='fpn_c2p2')(c2)])\n",
    "\n",
    "# Top-down pathway\n",
    "p5 = Conv2D(256, (3, 3), padding='same', name='fpn_p5')(p5)\n",
    "p4 = Conv2D(256, (3, 3), padding='same', name='fpn_p4')(p4)\n",
    "p3 = Conv2D(256, (3, 3), padding='same', name='fpn_p3')(p3)\n",
    "p2 = Conv2D(256, (3, 3), padding='same', name='fpn_p2')(p2)\n",
    "\n",
    "# Mask branch\n",
    "x = p5\n",
    "for i in range(4):\n",
    "    x = Conv2D(256, (3, 3), padding='same', name=f'mask_conv{i}')(x)\n",
    "    x = BatchNormalization(name=f'mask_bn{i}')(x)\n",
    "    x = Activation('relu')(x)\n",
    "mask_output = Conv2D(1, (1, 1), activation='sigmoid', name='mask_output')(x)\n",
    "\n",
    "# Class branch\n",
    "x = p5\n",
    "for i in range(4):\n",
    "    x = Conv2D(256, (3, 3), padding='same', name=f'class_conv{i}')(x)\n",
    "    x = BatchNormalization(name=f'class_bn{i}')(x)\n",
    "    x = Activation('relu')(x)\n",
    "class_output = Conv2D(NUM_CLASSES, (1, 1), activation='softmax', name='class_output')(x)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=[mask_output, class_output], name='FPN50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478ea64-0f61-4433-bdf3-97e7b8d89756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
